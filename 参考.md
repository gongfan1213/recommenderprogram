Chapter 13
# 第13章
# PDF翻译应用
文字翻译是自然语言处理的一个重要场景，应用非常广泛，比如，Google翻译、百度翻译、Bing翻译等技术都已发展得非常成熟，而且大部分浏览器中都内置了翻译工具，极大地降低了阅读外文的难度。但在PDF翻译这个细分领域中，由于PDF文件格式的特殊性，要对PDF进行翻译并保持原格式，这个过程其实比较复杂。本章针对PDF文件的特点，开发一个PDF翻译工具，利用Helsinki-NLP/opus-mt-en-zh模型和通用大语言模型的翻译能力，将PDF文件转化为HTML，再对文本翻译后回写，形成格式基本保持原样的HTML目标文件。
## 13.1 目标
翻译是自然语言处理领域的重要研究方向，包括文本翻译、语音翻译、文件翻译等应用。在大语言模型应用中，可以定制提示词，将一种语言的文本翻译成另一种语言的文本，如将文本中的中文翻译成英文，或将英文翻译成中文。
PDF是一种流式指令型格式，翻译时需要提取里面的文本内容，如果要求翻译的结果与原PDF保持同样的展示格式，则有一定的复杂性。本章设计一个基于大语言模型的PDF翻译工具，将PDF先转换为HTML格式，再用大语言模型翻译HTML里的文本内容，最后形成的HTML文件可以与原始PDF保持基本一样的显示格式。
## 13.2 原理
### 13.2.1 功能概要
本章开发一个PDF翻译工具，采取PDF转HTML的办法，将翻译功能应用到HTML文件，并将最终翻译的结果也回写到HTML中，其中翻译功能采用了大语言模型来实现。
该翻译工具的主要功能如下。
- 文件格式转换：将PDF转换为HTML格式。
- 文本翻译范围限定：在翻译的内容中过滤掉图表中的标注、源代码、数据公式等。
- 文本翻译模型：支持Helsinki-NLP/opus-mt-en-zh英译汉专用模型和通用大语言模型这两种不同的模型实现翻译任务。
- 大语言模型提示词定制：定制大语言模型的提示词，以便让大语言模型给出的翻译结果尽量准确。
### 13.2.2 系统架构

如图13-1所示，PDF翻译工具的工作流程：PDF原文件→HTML文件→解析出原始div标签→翻译div内容→替换div内容→保存翻译结果为HTML译文。

①PDF转HTML：这个过程由pdf2htmlEX工具完成。

②HTML解析：使用BeautifulSoup库装载HTML文件，将其解析成soup对象。

③搜索div：以div中的样式为条件，过滤出需要翻译的div内容。

④翻译div内容：调用Helsinki-NLP/opus-mt-en-zh或大语言模型进行翻译。

⑤替换div内容：将翻译后的文本回写到div中。

⑥保存HTML：将soup对象按原格式的呈现形式保存成HTML文件。

应用的架构见图13-1。

![image](https://github.com/user-attachments/assets/4d61e6ab-72f6-40a2-a6c5-ee7d2f789305)


图13-1 PDF翻译应用架构图
### 13.2.3 运行原理
#### 1. PDF转HTML
PDF文件不易编辑。为了便于翻译后保持格式不变，相对简单的方法是将PDF转成DOC或HTML。本例中选择了PDF转HTML的技术路线，具体转换工作由pdf2htmlEX应用程序完成。pdf2htmlEX是一个开源的PDF转HTML工具，源码见https://github.com/pdf2htmlEX/pdf2htmlEX，下载地址为https://soft.rubypdf.com/download/pdf2htmlex/pdf2htmlEX-win32-0.14.6-upx-with-poppler-data.zip。
pdf2htmlEX是一个命令行工具，运行以下命令进行转换工作。
pdf2htmlEX --zoom放大倍数 源pdf文件名 目标html文件名
如将test.pdf按1.5倍放大并保存为output.html，则命令如下。
```
pdf2htmlEX --zoom 1.5 test.pdf output.html
```
#### 2. HTML解析
BeautifulSoup是一个基于Python语言的HTML解析库，可以从HTML页面中提取特定的数据。BeautifulSoup库提供了API，用于从HTML页面中选择元素、添加新元素、修改元素内容和删除元素。BeautifulSoup库支持多种HTML解析模式，包括完整模式、指定模式和解析JSON数据。例如，用以下的代码解析output.html。解析的结果会存储在soup对象中，用于进一步的标签定位操作。
```
with open('output.html', 'r', encoding='utf-8') as file:
    soup = BeautifulSoup(file, 'html.parser')
```
#### 3. 搜索div标签
将PDF转成HTML后，文字标签由div和span组成。其中span嵌套在div中，起到了调节单词间距的作用。div标签的class属性标识了字号信息，以“t m数字……”，如下所示
```
<div class="t m0 x0 h2 y1 ff1 fs0 fc0 sc0 ls0 ws0">1
    <span class=" _ 0"></span>english sample1
    <span class=" _ 1"></span>english sample2
    <span class=" _ 1"></span>english sample3
    <span class=" _ 1"></span>english sample4
    <span class=" _ 1"></span>english sample5
</div>
```
为了获取所有需要翻译的div标签，可以进行如下调用，从而查询出所有class属性以“t”打头的div集合，翻译后回写。
```
divs = soup.find_all('div', class_="t")
```
#### 4. 翻译div内容
本章中，我们实践两种翻译模型来完成相应的翻译任务：一种是翻译专用的Helsinki-NLP/opus-mt-en-zh，另一种是通用大语言模型。两种模型各有优缺点，其对比见表13-1。

表13-1 专用模型与通用大模型在翻译能力上的对比

|  | Helsinki-NLP/opus-mt-en-zh | 通用大语言模型 |
| ---- | ---- | ---- |
| 算力要求 | 低（CPU和GPU均可） | 高（16GB及以上的GPU） |
| 翻译效率 | 快 | 慢 |
| 翻译准确度 | 较低 | 较高（需要定制提示词） |
| 调用难度 | 低 | 高 |
| 错误率 | 高（经常出现单词重复出现的情况，如“电子、电电电”） | 低 |

（1）Helsinki-NLP/opus-mt-en-zh模型调用
```
def translate_text(text):
    tokenizer = tokenizer(
        text, return_tensors='pt', truncation=True, \
        padding=True).to("cuda")
    tokenized_text['repetition_penalty'] = 1.2
    translation = model.generate(**tokenized_text)
    translated_text = tokenizer.batch_decode(
        translation, skip_special_tokens=True)
    return translated_text[0]
```
其中repetition_penalty为“惩罚率”参数，用于抑制单词重复出现。根据经验，这个参数值需要大于1。在程序调试中，可上下反复调整该参数来降低错误率。
（2）通用大语言模型调用
需要定制提示词，然后调用标准的openai v1/chat/completions接口。
```
messages = [
    {
        "role": "system",
        "content": "你是一个人工智能翻译工具，请翻译用户给出的英文。只输出翻译的最终结果，如果不能翻译，请原样输出"
    },
    {
        "role": "user",
        "content": 需要翻译的英文
    }
]
```
#### 5. 替换div内容
div中的span是用于对显示格式进行微小调整的，在翻译阶段用不到。调用“text = div.get_text(strip=True, separator=' ')”来获取已剔除span标签的所有文本，然后进行翻译，并将翻译的结果回写到div.string变量中。这样就把div中的span标签去掉了。由于中文与英文不同，中文的词和词中间不需要有空格，所以不影响显示。
#### 6. 保存HTML
将div中的英文内容翻译成中文后，soap对象也随之改变，将soap对象保存成文件即可。需要注意的是，不能用soap的prettify方法进行结果美化，否则后续保存的格式不正确，翻译结果无法正常显示。
with open(文件名, 'w', encoding='utf-8') as f:
    f.write(str(soup))
#### 7. 缺陷与改进措施
除了性能和准确率的问题外，上述实现示例中最大的不足在于文本的连续性和上下文方面。因为PDF文件是命令流格式，是按文本的具体坐标来控制打印的，所以一个完整的句子中的单词有可能被分拆。转换成HTML后，这些分拆的单词会被分到不同的div中，每个div中的单词都比较少。这样，单独对这些div中的文本进行翻译后组合成文章，就可能导致以下两个具体问题。
- 词不达意：机器翻译的痕迹非常重，因为是按单词而不是按完整的句子翻译的，也就谈不上联系上下文。
- 格式错位：由于翻译后的中文比英文短，结果中每行中文的长度都比原英文短，可读性差。
对此，可以探讨以下解决思路。
（1）相邻div归并的思路
在div的class属性格式中，如“<div class="t m0 x1 h3 y2 ff1 fs0 fc0 sc0 ls0 ws0">”，t表示(text文本)，m0指代一种变换矩阵，x1和y2指代字符的横纵坐标，h3表示字符高度，后面几项则表示字体、字号、阴影等。在相邻的两个div中，由于x和y不同，其class属性肯定是不同的，对其进行归纳合并会比较困难。即使忽略x和y值，只要字体相同就合并，判断过程也非常烦琐，需要大量的时间进行硬编码和测试，难度较大。下面给出源码供读者参考。
```
def merge_html_div():
    with open('output.html', 'r', encoding='utf-8') as file:
        soup = BeautifulSoup(file, 'html.parser')
        divs = soup.find_all('div', class_='t')
        i = 0
        while i < len(divs) - 1:
            if divs[i].class == divs[i + 1].class:
                new_div = soup.new_tag('div', **{'class': divs[i]['class']}) # 这里加字体匹配条件
                new_div.string = divs[i].string + divs[i + 1].string
                divs[i].string = ''
                divs[i + 1].extract()
                divs[i].insert_after(new_div)
                i += 1
            i += 1
    with open("output1.html", 'w', encoding='utf-8') as f:
        f.write(str(soup))
  ```
（2）PDF转成DOC的思路
由于生成的HTML使用样式严格控制每个词或短语的x和y坐标，该过程不易处理， 
